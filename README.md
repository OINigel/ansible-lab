ğŸš€ HPC â€“ Ansible Automation

This repository contains the full Ansible automation used to deploy and manage the Rocky HPC Cluster, including:

Core SLURM infrastructure (controller, backup controller, compute nodes)

Monitoring stack (Prometheus + Grafana)

System services (chrony, munge, sssd)

Dynamic inventory hooked into SLURM

Templated Slurm configuration (CPU/RAM autodiscovered)

The goal is to provide a fully reproducible, automated way to deploy & maintain an HPC cluster.

ğŸ“ Repository Structure
rocky-ansible/
â”œâ”€â”€ ansible.cfg                 # Global Ansible configuration
â”œâ”€â”€ inventories/
â”‚   â”œâ”€â”€ production/
â”‚   â”‚   â”œâ”€â”€ hosts.yml           # Static inventory (optional)
â”‚   â”‚   â”œâ”€â”€ group_vars/         # Cluster-wide vars
â”‚   â”‚   â””â”€â”€ host_vars/          # Per-node overrides
â”‚   â””â”€â”€ dynamic/
â”‚       â””â”€â”€ slurm_inventory.py  # Dynamic inventory powered by 'sinfo'
â”œâ”€â”€ roles/
â”‚   â”œâ”€â”€ common/                 # Base packages, chrony, etc.
â”‚   â”œâ”€â”€ munge/                  # Munge setup
â”‚   â”œâ”€â”€ node_exporter/          # Node exporter install
â”‚   â”œâ”€â”€ monitoring_server/      # Prometheus + Grafana
â”‚   â”œâ”€â”€ slurm_common/           # Shared Slurm templates
â”‚   â”œâ”€â”€ slurm_controller/
â”‚   â”œâ”€â”€ slurm_backup_controller/
â”‚   â””â”€â”€ slurm_compute/
â””â”€â”€ site.yml                    # Main playbook

ğŸ”§ Requirements

On the Ansible controller:

Python 3.8+

Ansible 2.15+

SSH access to all nodes (passwordless)

Git installed if pulling from GitHub

On the cluster nodes:

Linux (Rocky, Ubuntu, or Debian-based)

SSH enabled

Python installed (python3)

ğŸ—ï¸ How to Run a Deployment
1ï¸âƒ£ Test the dynamic inventory
ansible-inventory -i inventories/dynamic/slurm_inventory.py --graph


You should see something like:

@compute:
  |-- rocky-compute
  |-- rocky-compute2

2ï¸âƒ£ Run a dry-run (safe)
ansible-playbook -i inventories/dynamic/slurm_inventory.py site.yml --check

3ï¸âƒ£ Run for real
ansible-playbook -i inventories/dynamic/slurm_inventory.py site.yml

ğŸŒ Dynamic Inventory Overview

This cluster uses a custom dynamic inventory script:

inventories/dynamic/slurm_inventory.py


It queries:

ssh <controller> "sinfo -Nh -o '%N'"


â€¦then automatically populates:

compute

slurm_controller

slurm_backup_controller

monitor

If new compute nodes appear in SLURM, Ansible will automatically include them in deployments.

âš™ï¸ Slurm Configuration Template

Slurm uses a shared Jinja2 template:

roles/slurm_common/templates/slurm.conf.j2


It automatically fills:

Node names

IP addresses

CPU counts

Memory (MB)

Partition membership

Facts come from:

hostvars[node].ansible_facts


This means new compute nodes automatically generate valid entries in slurm.conf.

ğŸ“Š Monitoring Stack
Prometheus

Installed on: rocky-monitor

Accessible via:

http://<monitor-ip>:9090

Grafana

Installed on: rocky-monitor

UI:

http://<monitor-ip>:3000


Admin login defaults:

User: admin

Password: autogenerated or overridden in variables

ğŸ” Authentication (Munge + SSSD)

The repo configures:

Munge for SLURM authentication

SSSD for future FreeIPA / LDAP / Active Directory integration

Passwordless sudo via Ansible roles

These ensure secure & consistent identity handling across all nodes.

ğŸ—‚ï¸ GitHub Workflow
Add repo:
git remote add origin https://github.com/<username>/<repo>.git

Commit:
git add .
git commit -m "Initial commit"

Push:
git push -u origin main

ğŸ§© Customizing the Cluster
Add new compute nodes

Just add them to SLURM:

scontrol update NodeName=newnode State=RESUME


Then redeploy:

ansible-playbook -i inventories/dynamic/slurm_inventory.py site.yml


Ansible automatically:

Discovers CPU/RAM

Inserts NodeName into slurm.conf

Configures exporters

Starts slurmd

No manual editing needed.

ğŸ› ï¸ Troubleshooting
Can't SSH?

Check:

ansible.cfg â†’ remote_user

~/.ssh/id_rsa.pub exists on all nodes

Node exporter unreachable?
curl http://<node>:9100/metrics

Prometheus targets missing?
curl http://rocky-monitor:9090/api/v1/targets

Slurm not responding?
sudo systemctl status slurmctld
sudo systemctl status slurmd
sudo systemctl status munge

ğŸ“Œ Roadmap / TODOs

Alliance CCDB Integration (user & allocation sync)

IPA/FreeIPA for unified identity across cluster

GPU-aware partitions

Ceph integration

OOD (Open OnDemand) deployment

Federation with Alliance clusters (Narval, Beluga, Cedar, etc.)
